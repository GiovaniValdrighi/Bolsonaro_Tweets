{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"logistic_classification.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"4ZIHoDMRcj5F"},"source":["# Introduction to Data Science\n","\n","Propomos utilizar **Polaridade-Subjetividade**, junto a **Presença-Ausência** de um conjunto de temas em cada Tweet como parte dos preditores; junto a isso, a **Contagem de Retweets**, **Contagem de Favoritos**, **Presença de Mídias**, por exemplo.\n","\n","Propomos ainda usar um fator de retardo $k$ para caracterizar a influencia retardada das postagens. Caso o ajuste significativo ocorra sob um fator $k \\neq 0$, isto é, caso alterações relevantes sejam observadas após as postagens, e com coeficientes significativos, podemos discutir a influencia dos tweets como possível; caso o retardo seja nulo, ou pequeno, podemos discutir a existência de correlação sem causalidade.\n","\n","**Referencias Utilizadas**:\n","  * **Constantin Colonescu**. The Effects of Donald Trump’s Tweets on US Financial and Foreign Exchange Markets. Athens Journal of Business and Economics. Disponível em: www.athensjournals.gr/business/2018-1-X-Y-Colonescu.pdf"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"QBNjQXxpdAGu","colab":{}},"source":["import warnings\n","warnings.filterwarnings('ignore')\n","\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","\n","import matplotlib\n","import matplotlib.pyplot as plt\n","\n","from sklearn.linear_model import LogisticRegressionCV\n","from sklearn.preprocessing import PolynomialFeatures\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split\n","\n","from datetime import datetime\n","from datetime import timedelta\n","\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"cvD8umrJjfyo"},"source":["## Preparação dos dados\n","\n","Inclui as etapas de standandização, separação, e agregação segundo diferentes métodos. Primeiramente corrigimos os problemas de tipos de dados, depois transformamos as nossas variáveis qualitativas em **dummies**. Com isso agrupamos por data pegando a média de todas as colunas como função de agrupamento. Para essa primeira análise estaremos analisando os tweets de Jair Bolsonaro com relação ao câmbio do dólar.\n","\n","Primeiro passo é ler nossos dados."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"45HEQPAfSUhn","colab":{}},"source":["# ler os dados\n","tweets_data = pd.read_pickle(\"..//data//tweets//final_tweets_data.pkl\")\n","tweets_data = tweets_data.drop([\"full_text\", \"full_text_en\"], axis=1)\n","economic_data = pd.read_csv(\"..//data//economic_data//economic_time_series.csv\", sep=\";\", index_col=0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"5TDW6VXeSW1P"},"source":["Usamos um formato padrão de datas e ajustamos as séries temporais."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"C30dHi064aOm","colab":{}},"source":["# usar o formato de data padrão \"datetime\"\n","economic_data.index = pd.to_datetime(economic_data.index)\n","\n","tweets_data[\"date\"]  = tweets_data.year.apply(str) + \"-\"\n","tweets_data[\"date\"] += tweets_data.month.apply(str) + \"-\"\n","tweets_data[\"date\"] += tweets_data.day.apply(str)\n","tweets_data[\"date\"]  = pd.to_datetime(tweets_data[\"date\"])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"q9XwNmfYwCOa"},"source":["Adicionamos algumas *dummy-variables* convertendo um conjunto de variáveis qualitativas dos preditores em colunas numéricas. Também manteremos essa análise apenas para o twitter de jairbolsonaro."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"JEbNEe_VZJha","scrolled":true,"colab":{}},"source":["# filtrar nossos dados para o que queremos usar\n","tweets_data_cla = tweets_data.iloc[tweets_data.name.values=='jairbolsonaro',:]\n","tweets_data_cla = tweets_data_cla[['date','retweet_count','polarity','subjectivity','favorite_count','topic','media_type']]\n","# transformar as duas colunas em dummy\n","cols_to_dumm = [\"topic\",\"media_type\"]\n","for col in cols_to_dumm:\n","    dummies = pd.get_dummies(tweets_data_cla[col], prefix=col, drop_first=True)\n","    tweets_data_cla = pd.concat([tweets_data_cla.drop(col, axis=1), dummies], axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"irEEIL20ZLqC"},"source":["Para primeira análise, agregamos alguns dados com relação à data, de forma a pegar a média de todos os dados quantitativos naquela data e a soma de todos os dados qualitativos (ou dummy)."]},{"cell_type":"code","metadata":{"id":"_KLdYRprALne","colab_type":"code","colab":{}},"source":["# agregar por data usando a média nas seguintes colunas\n","dayly_agg_columns = [\"date\",'retweet_count','polarity','subjectivity','favorite_count']\n","dayly_agregated = tweets_data_cla[dayly_agg_columns].groupby(\"date\", as_index=False).mean()\n","\n","dayly_agregated.columns = [col + \"_mean\" for col in dayly_agg_columns]\n","dayly_agregated = dayly_agregated.set_index(\"date_mean\")\n","\n","# agregar por data usando a soma nas seguintes colunas\n","# as variáveis dummy são agregadas por soma para depois serem normalizadas de modo a somar um em cada categoria\n","dayly_agg_columns2 = ['date','media_type_photo','media_type_video']+['topic'+f'_{i}' for i in range(1,10)]\n","dayly_agregated2 = tweets_data_cla[dayly_agg_columns2].groupby(\"date\", as_index=False).sum()\n","\n","dayly_agregated2.columns = dayly_agg_columns2\n","dayly_agregated2 = dayly_agregated2.set_index(\"date\")\n","\n","# colocar os dados juntos\n","tweets_data_cla = pd.concat([dayly_agregated, dayly_agregated2], axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i3OIbUvkALnm","colab_type":"text"},"source":["Redimensionalizando nosso dados para média $0$ e desvio padrão $1$ e também tornando os tópicos e midías como porcentagem da soma total."]},{"cell_type":"code","metadata":{"id":"tqgfJtXoALnn","colab_type":"code","colab":{}},"source":["# transformando os dados quantitativos\n","cols_to_scale = ['retweet_count_mean','polarity_mean','subjectivity_mean','favorite_count_mean']\n","for col in cols_to_scale:\n","    tweets_data_cla[col] = StandardScaler().fit_transform(tweets_data_cla[col].values.reshape(-1,1))\n","\n","# transformando as dummies de modo a somar 1\n","for row in range(0,len(tweets_data_cla)):\n","    #media\n","    if tweets_data_cla.iloc[row,4:6].sum() != 0:\n","        tweets_data_cla.iloc[row,4:6] = tweets_data_cla.iloc[row,4:6]/tweets_data_cla.iloc[row,4:6].sum()\n","    #topic\n","    if tweets_data_cla.iloc[row,6:15].sum() != 0:\n","        tweets_data_cla.iloc[row,6:15] = tweets_data_cla.iloc[row,6:15]/tweets_data_cla.iloc[row,6:15].sum()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"FO1QRulbhtn6"},"source":["Lidando com o problema da compreensão das datas dos tweets contra os dos dados econômicos."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"DHGiKwR8fu95","colab":{}},"source":["# escolha os dados econômicos no mesmo alcance que os tweets\n","minD = tweets_data_cla.index.min()\n","maxD = tweets_data_cla.index.max()\n","\n","dt_day = timedelta(days=1)\n","economic_data = economic_data[economic_data.index >= minD - dt_day]\n","economic_data = economic_data[economic_data.index <= maxD]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"3I61_rxjPTfX"},"source":["Primeiramente, vamos pegar os index dos dados econômicos do câmbio do dólar, isto é, todas posições em que ele não é NaN."]},{"cell_type":"code","metadata":{"id":"ezpLK9ZsALoG","colab_type":"code","colab":{}},"source":["# pegue as datas em que temos o valor do dolar naquele dia e seus correspondentes index nos dados dos tweets\n","economic_index = 'dollar'\n","dates = economic_data.index[~np.isnan(economic_data[economic_index])]\n","dates_index_tweets = [tweets_data_cla.index[row] in dates for row in range(0,len(tweets_data_cla))]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_wQsbyQ3ALoN","colab_type":"text"},"source":["Agora, $k$ é o fator de quantas observações anterior impactam na atual (prever $X_{n+1}$ usando $X_{n-k},...,X_{n}$). Caso a nossa observação não possua $k$ observações anteriores, são usadas as que tem."]},{"cell_type":"code","metadata":{"scrolled":true,"id":"JHPwDmvBALoP","colab_type":"code","colab":{}},"source":["# passe pelos dados agregando os k dados anteriores\n","# transformamos os dados em np.array e depois de volta a dataframe devido à velocidade das operações\n","kMax = 2\n","model_data = tweets_data_cla.iloc[dates_index_tweets,:]\n","model_data_values = model_data.values\n","for date_ind in range(0,len(model_data.index)):\n","    row_values = model_data_values[date_ind,:]\n","    values_found = 1\n","    for k in range(1,kMax):\n","        cur_date = model_data.index[date_ind] - timedelta(days=k)\n","        if cur_date in tweets_data_cla.index:\n","            row_values = row_values + tweets_data_cla.values[list(tweets_data_cla.index).index(cur_date),:]\n","            values_found += 1\n","    model_data_values[date_ind,:] = row_values/values_found\n","model_data = pd.DataFrame(model_data_values,columns=model_data.columns, index = model_data.index)\n","# adicione os dados econômicos\n","model_data[economic_index] = economic_data[economic_index][~np.isnan(economic_data[economic_index])]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fJe7o4G4ALoW","colab_type":"text"},"source":["Para passar para o modelo nos falta apenas modificar os dados que queremos prever: ao invés de prever absolutamente o crescimento do índice, tentaremos modelar se houve crescimento ou não. "]},{"cell_type":"code","metadata":{"id":"yxN669t0ALoY","colab_type":"code","colab":{}},"source":["# transformar o crescimento do dólar em 1 caso aumentou 0 caso contrário\n","eco_data = model_data[economic_index].values\n","\n","for row in range(len(eco_data)-1,0,-1):\n","    eco_data[row] = int(eco_data[row]>=eco_data[row-1])\n","\n","model_data[economic_index] = eco_data\n","model_data = model_data.drop([model_data.index[0]])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K4JHACQ6ALog","colab_type":"text"},"source":["## Modelagem\n"]},{"cell_type":"markdown","metadata":{"id":"oK8fEJH2ALoh","colab_type":"text"},"source":["Separar os dados em treino e teste de modo que em cada um tenha a mesma quantidade de $1$'s e $0$'s."]},{"cell_type":"code","metadata":{"id":"xjApVT65ALoj","colab_type":"code","colab":{}},"source":["data_train, data_test = train_test_split(model_data, test_size = 0.2, stratify=model_data[economic_index],random_state=42)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KAbhgQDEALos","colab_type":"text"},"source":["Separar $X$ de $y$."]},{"cell_type":"code","metadata":{"id":"w8ttzsOfALou","colab_type":"code","colab":{}},"source":["x_train = data_train.drop(columns=economic_index)\n","x_test = data_test.drop(columns=economic_index)\n","y_train = np.array(data_train[economic_index])\n","y_test = np.array(data_test[economic_index])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EVp97xU9ALo1","colab_type":"text"},"source":["O primeiro modelo a ser criado é uma Regressão Logística usando Cross-Validation."]},{"cell_type":"code","metadata":{"scrolled":true,"id":"ejzUjCXDALo5","colab_type":"code","colab":{},"outputId":"8ae4b32f-b913-406d-ffb1-ed15c4bd56d3"},"source":["x_train_t = x_train.as_matrix()\n","y_train_t = np.array(y_train)\n","x_test_t = x_test.as_matrix()\n","y_test_t = np.array(y_test)\n","\n","lr = LogisticRegressionCV(solver='liblinear', multi_class='ovr',\n","                            penalty='l2', max_iter=100000, cv=10).fit(x_train_t,y_train_t)\n","\n","lr_train_accuracy = lr.score(x_train_t, y_train_t)\n","lr_test_accuracy = lr.score(x_test_t, y_test_t)\n","\n","scores = {'train': lr_train_accuracy, \n","          'test': lr_test_accuracy}\n","\n","print(scores)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'train': 0.5643153526970954, 'test': 0.5737704918032787}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"AzxdMIoVALpE","colab_type":"text"},"source":["Podemos perceber que o modelo teve uma boa acurácia nos dados de teste. Um outro modelo é a mesma Regressão Logística usando Cross-Validation só que, desta vez, adicionaremos termos polinomiais. No caso são adicionados os termos quadráticos dos preditores quantitativos e os termos de interação entre cada preditor quantitativo com cada preditor qualitativo."]},{"cell_type":"code","metadata":{"id":"MlBA_QISALpF","colab_type":"code","colab":{}},"source":["# columns_to_poly são os dados quantitativos e columns_to_interact são os qualitativos\n","columns_to_poly = ['retweet_count_mean', 'polarity_mean', 'subjectivity_mean','favorite_count_mean']\n","columns_to_interact = ['media_type_photo', 'media_type_video','topic_1', 'topic_2', 'topic_3',\n","                       'topic_4', 'topic_5', 'topic_6','topic_7', 'topic_8', 'topic_9']\n","\n","# nossos transformadores com termos de interação sim e não\n","poly_no_int = PolynomialFeatures(2, include_bias=False,interaction_only=False)\n","poly_int = PolynomialFeatures(2, include_bias=False,interaction_only=True)\n","\n","# transformar os dados X do treino e teste\n","dataframe_to_concat_train = []\n","dataframe_to_concat_test = []\n","for col in columns_to_poly:\n","    dataframe_to_concat_train.append(pd.DataFrame(poly_no_int.fit_transform(x_train[col].values.reshape(-1,1)),\n","                                           columns = [col,col+'**2']))\n","    dataframe_to_concat_test.append(pd.DataFrame(poly_no_int.fit_transform(x_test[col].values.reshape(-1,1)),\n","                                           columns = [col,col+'**2']))\n","\n","for col_poly in columns_to_poly:\n","    for col_int in columns_to_interact:\n","        k = poly_int.fit_transform(x_train[[col_poly,col_int]])\n","        dataframe_to_concat_train.append(pd.DataFrame(poly_int.fit_transform(x_train[[col_poly,col_int]])[:,2],\n","                                           columns = ['int_'+col_poly+'/'+col_int]))\n","        dataframe_to_concat_test.append(pd.DataFrame(poly_int.fit_transform(x_test[[col_poly,col_int]])[:,2],\n","                                           columns = ['int_'+col_poly+'/'+col_int]))        \n","\n","# adiciona as dummies\n","dataframe_to_concat_train.append(x_train[columns_to_interact].reset_index(drop=True))\n","dataframe_to_concat_test.append(x_test[columns_to_interact].reset_index(drop=True))\n","\n","# concatena todas nossas colunas\n","x_train_poly = pd.concat(dataframe_to_concat_train, axis=1)\n","x_test_poly = pd.concat(dataframe_to_concat_test, axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cAetxeHWALpM","colab_type":"text"},"source":["Com os dados modificados, podemos agora fitar o modelo."]},{"cell_type":"code","metadata":{"id":"fmAa5lMtALpO","colab_type":"code","colab":{},"outputId":"91034a10-bed7-4e64-c31d-a16e120f3b1a"},"source":["x_train_t_poly = x_train_poly.as_matrix()\n","x_test_t_poly = x_test_poly.as_matrix()\n","\n","lrPoly = LogisticRegressionCV(solver='liblinear', multi_class='ovr',\n","                            penalty='l2', max_iter=100000, cv=10).fit(x_train_t_poly,y_train_t)\n","\n","lr_train_accuracy = lrPoly.score(x_train_t_poly, y_train_t)\n","lr_test_accuracy = lrPoly.score(x_test_t_poly, y_test_t)\n","\n","scoresPoly = {'train': lr_train_accuracy, \n","          'test': lr_test_accuracy}\n","\n","print(scoresPoly)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'train': 0.5726141078838174, 'test': 0.5409836065573771}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"S5uwiegwALpV","colab_type":"text"},"source":["Podemos ver que o modelo teve uma performance melhor no treino do que o anterior, porém pior no teste. Isso provavelmente acontece devido ao overfitting que preditores polinomiais trazem.\n","\n","Duas outras medidas interessantes para se analisar nesse caso são:\n","\n","1) A porcentagem de valores $1$ (crescimento do índice) com relação ao total;\n","2) A acurácia do modelo na classe $0$ (geralmente é a não dominante) no treino e teste;"]},{"cell_type":"code","metadata":{"id":"dMdo3YYGALpX","colab_type":"code","colab":{},"outputId":"31f56e8c-65ed-47bc-8d62-07335ece7d86"},"source":["lr_train_class_accuracy = lrPoly.score(x_train_t_poly[y_train_t==0,:], y_train_t[y_train_t==0])\n","lr_test_class_accuracy = lrPoly.score(x_test_t_poly[y_test_t==0,:], y_test_t[y_test_t==0])\n","value_percentage = model_data[economic_index].sum()/len(model_data[economic_index])\n","print(\"Acurácia na classe 0 do treino:\",lr_train_class_accuracy,\"\\nAcurácia na classe 0 do teste:\",lr_test_class_accuracy,\"\\nPorcentagem de 1s\",value_percentage)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Acurácia na classe 0 do treino: 0.23478260869565218 \n","Acurácia na classe 0 do teste: 0.3103448275862069 \n","Porcentagem de 1s 0.5231788079470199\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9SSRGBoqALpe","colab_type":"text"},"source":["Podemos ver que nosso modelo possue baixa acurácia em ambos treino e teste na classe $0$ (índice decresceu). Também é possível analisar que a acurácia de um modelo que só \"chutasse\" 1's seria quase a mesma do nosso modelo logístico, indicando que o modelo não conseguiu uma performance muito alta.\n","\n","Essas medidas correspondem apenas ao do modelo polinomial, mas poderíamos calcular também para o linear. Entretanto, como queremos analisar diversos outros fatores além destes, podemos partir para um algoritmo mais geral, isto é, podemos executar o mesmo procedimento para cada índice econômico diário, diversos valores de retardo $k$ e para cada um dos membros da família Bolsonaro:"]},{"cell_type":"code","metadata":{"id":"IJUK-xI8ALpf","colab_type":"code","colab":{},"outputId":"b7acc3a4-0b0b-43cf-fc65-d379dfda967d"},"source":["results = []\n","for economic_index in ['selic_meta','international_reserve','cdi','dollar']:\n","    for kMax in range(0,5):\n","        dates = economic_data.index[~np.isnan(economic_data[economic_index])]\n","        dates_index_tweets = [tweets_data_cla.index[row] in dates for row in range(0,len(tweets_data_cla))]\n","        model_data = tweets_data_cla.iloc[dates_index_tweets,:]\n","        model_data_values = model_data.values\n","        for date_ind in range(0,len(model_data.index)):\n","            row_values = model_data_values[date_ind,:]\n","            values_found = 1\n","            for k in range(1,kMax):\n","                cur_date = model_data.index[date_ind] - timedelta(days=k)\n","                if cur_date in tweets_data_cla.index:\n","                    row_values = row_values + tweets_data_cla.values[list(tweets_data_cla.index).index(cur_date),:]\n","                    values_found += 1\n","            model_data_values[date_ind,:] = row_values/values_found\n","        model_data = pd.DataFrame(model_data_values,columns=model_data.columns, index = model_data.index)\n","        model_data[economic_index] = economic_data[economic_index][~np.isnan(economic_data[economic_index])]\n","        eco_data = model_data[economic_index].values\n","        for row in range(len(eco_data)-1,0,-1):\n","            eco_data[row] = int(eco_data[row]>=eco_data[row-1])\n","        model_data[economic_index] = eco_data\n","        model_data = model_data.drop([model_data.index[0]])\n","        data_train, data_test = train_test_split(model_data,\n","                                                 test_size = 0.2, \n","                                                 stratify=model_data[economic_index],random_state=42)\n","        x_train = data_train.drop(columns=economic_index)\n","        x_test = data_test.drop(columns=economic_index)\n","        y_train = np.array(data_train[economic_index])\n","        y_test = np.array(data_test[economic_index])        \n","        x_train_t = x_train.as_matrix()\n","        y_train_t = np.array(y_train)\n","        x_test_t = x_test.as_matrix()\n","        y_test_t = np.array(y_test)\n","        lr = LogisticRegressionCV(solver='liblinear', multi_class='ovr',\n","                            penalty='l2', max_iter=100000, cv=10).fit(x_train_t,y_train_t)\n","        lr_train_accuracy = lr.score(x_train_t, y_train_t)\n","        lr_test_accuracy = lr.score(x_test_t, y_test_t)\n","        lr_train_class_accuracy = lr.score(x_train_t[y_train_t==0,:], y_train_t[y_train_t==0])\n","        lr_test_class_accuracy = lr.score(x_test_t[y_test_t==0,:], y_test_t[y_test_t==0])\n","        scores = {'train': lr_train_accuracy,\n","                  'test': lr_test_accuracy,\n","                  'class_train': lr_train_class_accuracy,\n","                  'class_test': lr_test_class_accuracy}\n","        columns_to_poly = ['retweet_count_mean', 'polarity_mean', 'subjectivity_mean','favorite_count_mean']\n","        columns_to_interact = ['media_type_photo', 'media_type_video','topic_1', 'topic_2', 'topic_3',\n","                               'topic_4', 'topic_5', 'topic_6','topic_7', 'topic_8', 'topic_9']\n","        poly_no_int = PolynomialFeatures(2, include_bias=False,interaction_only=False)\n","        poly_int = PolynomialFeatures(2, include_bias=False,interaction_only=True)\n","        dataframe_to_concat_train = []\n","        dataframe_to_concat_test = []\n","        for col in columns_to_poly:\n","            dataframe_to_concat_train.append(pd.DataFrame(poly_no_int.fit_transform(x_train[col].values.reshape(-1,1)),\n","                                                   columns = [col,col+'**2']))\n","            dataframe_to_concat_test.append(pd.DataFrame(poly_no_int.fit_transform(x_test[col].values.reshape(-1,1)),\n","                                                   columns = [col,col+'**2']))\n","        for col_poly in columns_to_poly:\n","            for col_int in columns_to_interact:\n","                k = poly_int.fit_transform(x_train[[col_poly,col_int]])\n","                dataframe_to_concat_train.append(pd.DataFrame(poly_int.fit_transform(x_train[[col_poly,col_int]])[:,2],\n","                                                   columns = ['int_'+col_poly+'/'+col_int]))\n","                dataframe_to_concat_test.append(pd.DataFrame(poly_int.fit_transform(x_test[[col_poly,col_int]])[:,2],\n","                                                   columns = ['int_'+col_poly+'/'+col_int])) \n","        dataframe_to_concat_train.append(x_train[columns_to_interact].reset_index(drop=True))\n","        dataframe_to_concat_test.append(x_test[columns_to_interact].reset_index(drop=True))\n","        x_train_poly = pd.concat(dataframe_to_concat_train, axis=1)\n","        x_test_poly = pd.concat(dataframe_to_concat_test, axis=1)\n","        x_train_t_poly = x_train_poly.as_matrix()\n","        x_test_t_poly = x_test_poly.as_matrix()\n","        lrPoly = LogisticRegressionCV(solver='liblinear', multi_class='ovr',\n","                                    penalty='l2', max_iter=100000, cv=10).fit(x_train_t_poly,y_train_t)\n","        lr_train_accuracy = lrPoly.score(x_train_t_poly, y_train_t)\n","        lr_test_accuracy = lrPoly.score(x_test_t_poly, y_test_t)\n","        lr_train_class_accuracy = lrPoly.score(x_train_t_poly[y_train_t==0,:], y_train_t[y_train_t==0])\n","        lr_test_class_accuracy = lrPoly.score(x_test_t_poly[y_test_t==0,:], y_test_t[y_test_t==0])\n","        scoresPoly = {'train': lr_train_accuracy, \n","                      'test': lr_test_accuracy,\n","                      'class_train': lr_train_class_accuracy,\n","                      'class_test': lr_test_class_accuracy}        \n","        results.append([economic_index,kMax, \n","                       model_data[economic_index].sum()/len(model_data[economic_index]),\n","                       scores['train'],scores['test'], \n","                       scores['class_train'],scores['class_test'],\n","                       scoresPoly['train'],scoresPoly['test'],\n","                       scoresPoly['class_train'],scoresPoly['class_test']])\n","results = pd.DataFrame(results,columns=['eco_ind','k','per_y1','LogLin_acu_tr','LogLin_acu_te',\n","                                       'LogLin_cla0_acu_tr','LogLin_cla0_acu_te','LogPoly_acu_tr','LogPoly_acu_te',\n","                                       'LogPoly_cla0_acu_tr','LogPoly_cla0_acu_te'])\n","results"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>eco_ind</th>\n","      <th>k</th>\n","      <th>per_y1</th>\n","      <th>LogLin_acu_tr</th>\n","      <th>LogLin_acu_te</th>\n","      <th>LogLin_cla0_acu_tr</th>\n","      <th>LogLin_cla0_acu_te</th>\n","      <th>LogPoly_acu_tr</th>\n","      <th>LogPoly_acu_te</th>\n","      <th>LogPoly_cla0_acu_tr</th>\n","      <th>LogPoly_cla0_acu_te</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>selic_meta</td>\n","      <td>0</td>\n","      <td>0.981735</td>\n","      <td>0.982857</td>\n","      <td>0.977273</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.982857</td>\n","      <td>0.977273</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>selic_meta</td>\n","      <td>1</td>\n","      <td>0.981735</td>\n","      <td>0.982857</td>\n","      <td>0.977273</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.982857</td>\n","      <td>0.977273</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>selic_meta</td>\n","      <td>2</td>\n","      <td>0.981735</td>\n","      <td>0.982857</td>\n","      <td>0.977273</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.982857</td>\n","      <td>0.977273</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>selic_meta</td>\n","      <td>3</td>\n","      <td>0.981735</td>\n","      <td>0.982857</td>\n","      <td>0.977273</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.982857</td>\n","      <td>0.977273</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>selic_meta</td>\n","      <td>4</td>\n","      <td>0.981735</td>\n","      <td>0.982857</td>\n","      <td>0.977273</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.982857</td>\n","      <td>0.977273</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>international_reserve</td>\n","      <td>0</td>\n","      <td>0.496689</td>\n","      <td>0.564315</td>\n","      <td>0.606557</td>\n","      <td>0.553719</td>\n","      <td>0.645161</td>\n","      <td>0.605809</td>\n","      <td>0.508197</td>\n","      <td>0.595041</td>\n","      <td>0.580645</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>international_reserve</td>\n","      <td>1</td>\n","      <td>0.496689</td>\n","      <td>0.564315</td>\n","      <td>0.606557</td>\n","      <td>0.553719</td>\n","      <td>0.645161</td>\n","      <td>0.605809</td>\n","      <td>0.508197</td>\n","      <td>0.595041</td>\n","      <td>0.580645</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>international_reserve</td>\n","      <td>2</td>\n","      <td>0.496689</td>\n","      <td>0.593361</td>\n","      <td>0.524590</td>\n","      <td>0.570248</td>\n","      <td>0.580645</td>\n","      <td>0.618257</td>\n","      <td>0.590164</td>\n","      <td>0.561983</td>\n","      <td>0.612903</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>international_reserve</td>\n","      <td>3</td>\n","      <td>0.496689</td>\n","      <td>0.605809</td>\n","      <td>0.606557</td>\n","      <td>0.595041</td>\n","      <td>0.612903</td>\n","      <td>0.597510</td>\n","      <td>0.639344</td>\n","      <td>0.520661</td>\n","      <td>0.612903</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>international_reserve</td>\n","      <td>4</td>\n","      <td>0.496689</td>\n","      <td>0.572614</td>\n","      <td>0.590164</td>\n","      <td>0.570248</td>\n","      <td>0.580645</td>\n","      <td>0.734440</td>\n","      <td>0.508197</td>\n","      <td>0.752066</td>\n","      <td>0.419355</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>cdi</td>\n","      <td>0</td>\n","      <td>0.973510</td>\n","      <td>0.975104</td>\n","      <td>0.967213</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.975104</td>\n","      <td>0.967213</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>cdi</td>\n","      <td>1</td>\n","      <td>0.973510</td>\n","      <td>0.975104</td>\n","      <td>0.967213</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.975104</td>\n","      <td>0.967213</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>cdi</td>\n","      <td>2</td>\n","      <td>0.973510</td>\n","      <td>0.975104</td>\n","      <td>0.967213</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.975104</td>\n","      <td>0.967213</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>cdi</td>\n","      <td>3</td>\n","      <td>0.973510</td>\n","      <td>0.975104</td>\n","      <td>0.967213</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.975104</td>\n","      <td>0.967213</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>cdi</td>\n","      <td>4</td>\n","      <td>0.973510</td>\n","      <td>0.975104</td>\n","      <td>0.967213</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.975104</td>\n","      <td>0.967213</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>dollar</td>\n","      <td>0</td>\n","      <td>0.523179</td>\n","      <td>0.597510</td>\n","      <td>0.540984</td>\n","      <td>0.460870</td>\n","      <td>0.448276</td>\n","      <td>0.709544</td>\n","      <td>0.524590</td>\n","      <td>0.686957</td>\n","      <td>0.413793</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>dollar</td>\n","      <td>1</td>\n","      <td>0.523179</td>\n","      <td>0.597510</td>\n","      <td>0.540984</td>\n","      <td>0.460870</td>\n","      <td>0.448276</td>\n","      <td>0.709544</td>\n","      <td>0.524590</td>\n","      <td>0.686957</td>\n","      <td>0.413793</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>dollar</td>\n","      <td>2</td>\n","      <td>0.523179</td>\n","      <td>0.564315</td>\n","      <td>0.573770</td>\n","      <td>0.226087</td>\n","      <td>0.310345</td>\n","      <td>0.572614</td>\n","      <td>0.540984</td>\n","      <td>0.234783</td>\n","      <td>0.310345</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>dollar</td>\n","      <td>3</td>\n","      <td>0.523179</td>\n","      <td>0.564315</td>\n","      <td>0.557377</td>\n","      <td>0.226087</td>\n","      <td>0.241379</td>\n","      <td>0.589212</td>\n","      <td>0.590164</td>\n","      <td>0.295652</td>\n","      <td>0.310345</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>dollar</td>\n","      <td>4</td>\n","      <td>0.523179</td>\n","      <td>0.651452</td>\n","      <td>0.524590</td>\n","      <td>0.573913</td>\n","      <td>0.379310</td>\n","      <td>0.564315</td>\n","      <td>0.540984</td>\n","      <td>0.226087</td>\n","      <td>0.241379</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                  eco_ind  k    per_y1  LogLin_acu_tr  LogLin_acu_te  \\\n","0              selic_meta  0  0.981735       0.982857       0.977273   \n","1              selic_meta  1  0.981735       0.982857       0.977273   \n","2              selic_meta  2  0.981735       0.982857       0.977273   \n","3              selic_meta  3  0.981735       0.982857       0.977273   \n","4              selic_meta  4  0.981735       0.982857       0.977273   \n","5   international_reserve  0  0.496689       0.564315       0.606557   \n","6   international_reserve  1  0.496689       0.564315       0.606557   \n","7   international_reserve  2  0.496689       0.593361       0.524590   \n","8   international_reserve  3  0.496689       0.605809       0.606557   \n","9   international_reserve  4  0.496689       0.572614       0.590164   \n","10                    cdi  0  0.973510       0.975104       0.967213   \n","11                    cdi  1  0.973510       0.975104       0.967213   \n","12                    cdi  2  0.973510       0.975104       0.967213   \n","13                    cdi  3  0.973510       0.975104       0.967213   \n","14                    cdi  4  0.973510       0.975104       0.967213   \n","15                 dollar  0  0.523179       0.597510       0.540984   \n","16                 dollar  1  0.523179       0.597510       0.540984   \n","17                 dollar  2  0.523179       0.564315       0.573770   \n","18                 dollar  3  0.523179       0.564315       0.557377   \n","19                 dollar  4  0.523179       0.651452       0.524590   \n","\n","    LogLin_cla0_acu_tr  LogLin_cla0_acu_te  LogPoly_acu_tr  LogPoly_acu_te  \\\n","0             0.000000            0.000000        0.982857        0.977273   \n","1             0.000000            0.000000        0.982857        0.977273   \n","2             0.000000            0.000000        0.982857        0.977273   \n","3             0.000000            0.000000        0.982857        0.977273   \n","4             0.000000            0.000000        0.982857        0.977273   \n","5             0.553719            0.645161        0.605809        0.508197   \n","6             0.553719            0.645161        0.605809        0.508197   \n","7             0.570248            0.580645        0.618257        0.590164   \n","8             0.595041            0.612903        0.597510        0.639344   \n","9             0.570248            0.580645        0.734440        0.508197   \n","10            0.000000            0.000000        0.975104        0.967213   \n","11            0.000000            0.000000        0.975104        0.967213   \n","12            0.000000            0.000000        0.975104        0.967213   \n","13            0.000000            0.000000        0.975104        0.967213   \n","14            0.000000            0.000000        0.975104        0.967213   \n","15            0.460870            0.448276        0.709544        0.524590   \n","16            0.460870            0.448276        0.709544        0.524590   \n","17            0.226087            0.310345        0.572614        0.540984   \n","18            0.226087            0.241379        0.589212        0.590164   \n","19            0.573913            0.379310        0.564315        0.540984   \n","\n","    LogPoly_cla0_acu_tr  LogPoly_cla0_acu_te  \n","0              0.000000             0.000000  \n","1              0.000000             0.000000  \n","2              0.000000             0.000000  \n","3              0.000000             0.000000  \n","4              0.000000             0.000000  \n","5              0.595041             0.580645  \n","6              0.595041             0.580645  \n","7              0.561983             0.612903  \n","8              0.520661             0.612903  \n","9              0.752066             0.419355  \n","10             0.000000             0.000000  \n","11             0.000000             0.000000  \n","12             0.000000             0.000000  \n","13             0.000000             0.000000  \n","14             0.000000             0.000000  \n","15             0.686957             0.413793  \n","16             0.686957             0.413793  \n","17             0.234783             0.310345  \n","18             0.295652             0.310345  \n","19             0.226087             0.241379  "]},"metadata":{"tags":[]},"execution_count":71}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"q5qCWHF0tQyr","colab":{}},"source":["O código acima é apenas tudo que fizermos anteriormente concatenados de forma à executarmos a mesma análise nos outros índices e valores de $k$. Uma análise a primeira instância é de que:\n","\n","1. Os índices econômicos selic_meta e cdi possuem 97% de aumento nos dados total, de modo que nosso modelo falhou pois classifica qualquer entrada em $1$;\n","2. Para a reserva internacional, $k=3$ nos forneceu a maior acurácia no teste em ambos modelos (linear e polinomial) com $0.612903$ e $0.639344$, respectivamente.\n","3. Para o câmbio do dólar, $k=2$ nos forneceu a maior acurácia no teste no modelo $1$, com $0.573770$ enquanto $k=3$ no modelo $2$ nos deu $0.590164$."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q83vMWfQALpt","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}