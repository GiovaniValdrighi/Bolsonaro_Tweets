{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4ZIHoDMRcj5F"
   },
   "source": [
    "# Classificação Logística\n",
    "\n",
    "Propomos utilizar **Polaridade-Subjetividade**, junto a **Presença-Ausência** de um conjunto de temas em cada Tweet como parte dos preditores; junto a isso, a **Contagem de Retweets**, **Contagem de Favoritos**, **Presença de Mídias**, por exemplo.\n",
    "\n",
    "Propomos ainda usar um fator de retardo $k$ para caracterizar a influencia retardada das postagens. Caso o ajuste significativo ocorra sob um fator $k \\neq 0$, isto é, caso alterações relevantes sejam observadas após as postagens, e com coeficientes significativos, podemos discutir a influencia dos tweets como possível; caso o retardo seja nulo, ou pequeno, podemos discutir a existência de correlação sem causalidade.\n",
    "\n",
    "**Referencias Utilizadas**:\n",
    "  * **Constantin Colonescu**. The Effects of Donald Trump’s Tweets on US Financial and Foreign Exchange Markets. Athens Journal of Business and Economics. Disponível em: www.athensjournals.gr/business/2018-1-X-Y-Colonescu.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QBNjQXxpdAGu"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from scipy.stats import norm\n",
    "\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cvD8umrJjfyo"
   },
   "source": [
    "## Preparação dos dados\n",
    "\n",
    "Inclui as etapas de standandização, separação, e agregação segundo diferentes métodos. Primeiramente corrigimos os problemas de tipos de dados, depois transformamos as nossas variáveis qualitativas em **dummies**. Com isso agrupamos por data pegando a média de todas as colunas como função de agrupamento. Para essa primeira análise estaremos analisando os tweets de Jair Bolsonaro com relação ao câmbio do dólar.\n",
    "\n",
    "Primeiro passo é ler nossos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "45HEQPAfSUhn"
   },
   "outputs": [],
   "source": [
    "# ler os dados\n",
    "tweets_data = pd.read_pickle(\"..//data//tweets//final_tweets_data.pkl\")\n",
    "tweets_data = tweets_data.drop([\"full_text\", \"full_text_en\"], axis=1)\n",
    "economic_data = pd.read_csv(\"..//data//economic_data//economic_time_series.csv\", sep=\";\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5TDW6VXeSW1P"
   },
   "source": [
    "Usamos um formato padrão de datas e ajustamos as séries temporais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C30dHi064aOm"
   },
   "outputs": [],
   "source": [
    "# usar o formato de data padrão \"datetime\"\n",
    "economic_data.index = pd.to_datetime(economic_data.index)\n",
    "\n",
    "tweets_data[\"date\"]  = tweets_data.year.apply(str) + \"-\"\n",
    "tweets_data[\"date\"] += tweets_data.month.apply(str) + \"-\"\n",
    "tweets_data[\"date\"] += tweets_data.day.apply(str)\n",
    "tweets_data[\"date\"]  = pd.to_datetime(tweets_data[\"date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q9XwNmfYwCOa"
   },
   "source": [
    "Adicionamos algumas *dummy-variables* convertendo um conjunto de variáveis qualitativas dos preditores em colunas numéricas. Também manteremos essa análise apenas para o twitter de jairbolsonaro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JEbNEe_VZJha",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# filtrar nossos dados para o que queremos usar\n",
    "tweets_data_cla = tweets_data.iloc[tweets_data.name.values=='jairbolsonaro',:]\n",
    "tweets_data_cla = tweets_data_cla[['date','retweet_count','polarity','subjectivity','favorite_count','topic','media_type']]\n",
    "tweets_data_cla['media_type'] = tweets_data_cla['media_type'].fillna('none')\n",
    "# transformar as duas colunas em dummy\n",
    "cols_to_dumm = [\"topic\",\"media_type\"]\n",
    "for col in cols_to_dumm:\n",
    "    dummies = pd.get_dummies(tweets_data_cla[col], prefix=col, drop_first=True)\n",
    "    tweets_data_cla = pd.concat([tweets_data_cla.drop(col, axis=1), dummies], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "irEEIL20ZLqC"
   },
   "source": [
    "Para primeira análise, agregamos alguns dados com relação à data, de forma a pegar a média de todos os dados quantitativos naquela data e a soma de todos os dados qualitativos (ou dummy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_KLdYRprALne"
   },
   "outputs": [],
   "source": [
    "# agregar por data usando a média nas seguintes colunas\n",
    "dayly_agg_columns = [\"date\",'retweet_count','polarity','subjectivity','favorite_count']\n",
    "dayly_agregated = tweets_data_cla[dayly_agg_columns].groupby(\"date\", as_index=False).mean()\n",
    "\n",
    "dayly_agregated.columns = [col + \"_mean\" for col in dayly_agg_columns]\n",
    "dayly_agregated = dayly_agregated.set_index(\"date_mean\")\n",
    "\n",
    "# agregar por data usando a soma nas seguintes colunas\n",
    "# as variáveis dummy são agregadas por soma para depois serem normalizadas de modo a somar um em cada categoria\n",
    "dayly_agg_columns2 = ['date','media_type_photo','media_type_video']+['topic'+f'_{i}' for i in range(1,10)]\n",
    "dayly_agregated2 = tweets_data_cla[dayly_agg_columns2].groupby(\"date\", as_index=False).sum()\n",
    "\n",
    "dayly_agregated2.columns = dayly_agg_columns2\n",
    "dayly_agregated2 = dayly_agregated2.set_index(\"date\")\n",
    "\n",
    "# colocar os dados juntos\n",
    "tweets_data_cla = pd.concat([dayly_agregated, dayly_agregated2], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i3OIbUvkALnm"
   },
   "source": [
    "Redimensionalizando nosso dados para média $0$ e desvio padrão $1$ e também tornando os tópicos e midías como porcentagem da soma total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tqgfJtXoALnn"
   },
   "outputs": [],
   "source": [
    "# transformando os dados quantitativos\n",
    "cols_to_scale = ['retweet_count_mean','polarity_mean','subjectivity_mean','favorite_count_mean']\n",
    "for col in cols_to_scale:\n",
    "    tweets_data_cla[col] = StandardScaler().fit_transform(tweets_data_cla[col].values.reshape(-1,1))\n",
    "\n",
    "# transformando as dummies de modo a somar 1\n",
    "for row in range(0,len(tweets_data_cla)):\n",
    "    #media\n",
    "    if tweets_data_cla.iloc[row,4:6].sum() != 0:\n",
    "        tweets_data_cla.iloc[row,4:6] = tweets_data_cla.iloc[row,4:6]/tweets_data_cla.iloc[row,4:6].sum()\n",
    "    #topic\n",
    "    if tweets_data_cla.iloc[row,6:15].sum() != 0:\n",
    "        tweets_data_cla.iloc[row,6:15] = tweets_data_cla.iloc[row,6:15]/tweets_data_cla.iloc[row,6:15].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FO1QRulbhtn6"
   },
   "source": [
    "Lidando com o problema da compreensão das datas dos tweets contra os dos dados econômicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DHGiKwR8fu95"
   },
   "outputs": [],
   "source": [
    "# escolha os dados econômicos no mesmo alcance que os tweets\n",
    "minD = tweets_data_cla.index.min()\n",
    "maxD = tweets_data_cla.index.max()\n",
    "\n",
    "dt_day = timedelta(days=1)\n",
    "economic_data = economic_data[economic_data.index >= minD - dt_day]\n",
    "economic_data = economic_data[economic_data.index <= maxD]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3I61_rxjPTfX"
   },
   "source": [
    "Primeiramente, vamos pegar os index dos dados econômicos do câmbio do dólar, isto é, todas posições em que ele não é NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ezpLK9ZsALoG"
   },
   "outputs": [],
   "source": [
    "# pegue as datas em que temos o valor do dolar naquele dia e seus correspondentes index nos dados dos tweets\n",
    "economic_index = 'dollar'\n",
    "dates = economic_data.index[~np.isnan(economic_data[economic_index])]\n",
    "dates_index_tweets = [tweets_data_cla.index[row] in dates for row in range(0,len(tweets_data_cla))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_wQsbyQ3ALoN"
   },
   "source": [
    "Agora, $k$ é o fator de quantas observações anterior impactam na atual (prever $X_{n+1}$ usando $X_{n-k},...,X_{n}$). Caso a nossa observação não possua $k$ observações anteriores, são usadas as que tem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JHPwDmvBALoP",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# passe pelos dados agregando os k dados anteriores\n",
    "# transformamos os dados em np.array e depois de volta a dataframe devido à velocidade das operações\n",
    "kMax = 2\n",
    "model_data = tweets_data_cla.iloc[dates_index_tweets,:]\n",
    "model_data_values = model_data.values\n",
    "for date_ind in range(0,len(model_data.index)):\n",
    "    row_values = model_data_values[date_ind,:]\n",
    "    values_found = 1\n",
    "    for k in range(1,kMax+1):\n",
    "        cur_date = model_data.index[date_ind] - timedelta(days=k)\n",
    "        if cur_date in tweets_data_cla.index:\n",
    "            row_values = row_values + tweets_data_cla.values[list(tweets_data_cla.index).index(cur_date),:]\n",
    "            values_found += 1\n",
    "    model_data_values[date_ind,:] = row_values/values_found\n",
    "model_data = pd.DataFrame(model_data_values,columns=model_data.columns, index = model_data.index)\n",
    "# adicione os dados econômicos\n",
    "model_data[economic_index] = economic_data[economic_index][~np.isnan(economic_data[economic_index])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fJe7o4G4ALoW"
   },
   "source": [
    "Para passar para o modelo nos falta apenas modificar os dados que queremos prever: ao invés de prever absolutamente o crescimento do índice, tentaremos modelar se houve crescimento ou não. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yxN669t0ALoY"
   },
   "outputs": [],
   "source": [
    "# transformar o crescimento do dólar em 1 caso aumentou 0 caso contrário\n",
    "eco_data = model_data[economic_index].values\n",
    "\n",
    "for row in range(len(eco_data)-1,0,-1):\n",
    "    eco_data[row] = int(eco_data[row]>=eco_data[row-1])\n",
    "\n",
    "model_data[economic_index] = eco_data\n",
    "model_data = model_data.drop([model_data.index[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K4JHACQ6ALog"
   },
   "source": [
    "## Modelagem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oK8fEJH2ALoh"
   },
   "source": [
    "Separar os dados em treino e teste de modo que em cada um tenha a mesma quantidade de $1$'s e $0$'s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xjApVT65ALoj"
   },
   "outputs": [],
   "source": [
    "data_train, data_test = train_test_split(model_data, test_size = 0.2, stratify=model_data[economic_index],random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KAbhgQDEALos"
   },
   "source": [
    "Separar $X$ de $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w8ttzsOfALou"
   },
   "outputs": [],
   "source": [
    "x_train = data_train.drop(columns=economic_index)\n",
    "x_test = data_test.drop(columns=economic_index)\n",
    "y_train = np.array(data_train[economic_index])\n",
    "y_test = np.array(data_test[economic_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EVp97xU9ALo1"
   },
   "source": [
    "O primeiro modelo a ser criado é uma Regressão Logística usando Cross-Validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ejzUjCXDALo5",
    "outputId": "8ae4b32f-b913-406d-ffb1-ed15c4bd56d3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 0.5643153526970954, 'test': 0.5573770491803278}\n"
     ]
    }
   ],
   "source": [
    "x_train_t = x_train.as_matrix()\n",
    "y_train_t = np.array(y_train)\n",
    "x_test_t = x_test.as_matrix()\n",
    "y_test_t = np.array(y_test)\n",
    "\n",
    "lr = LogisticRegressionCV(solver='liblinear', multi_class='ovr',\n",
    "                            penalty='l2', max_iter=100000, cv=10).fit(x_train_t,y_train_t)\n",
    "\n",
    "lr_train_accuracy = lr.score(x_train_t, y_train_t)\n",
    "lr_test_accuracy = lr.score(x_test_t, y_test_t)\n",
    "\n",
    "scores = {'train': lr_train_accuracy, \n",
    "          'test': lr_test_accuracy}\n",
    "\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AzxdMIoVALpE"
   },
   "source": [
    "Podemos perceber que o modelo teve uma boa acurácia nos dados de teste. Um outro modelo é a mesma Regressão Logística usando Cross-Validation só que, desta vez, adicionaremos termos polinomiais. No caso são adicionados os termos quadráticos dos preditores quantitativos e os termos de interação entre cada preditor quantitativo com cada preditor qualitativo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MlBA_QISALpF"
   },
   "outputs": [],
   "source": [
    "# columns_to_poly são os dados quantitativos e columns_to_interact são os qualitativos\n",
    "columns_to_poly = ['retweet_count_mean', 'polarity_mean', 'subjectivity_mean','favorite_count_mean']\n",
    "columns_to_interact = ['media_type_photo', 'media_type_video','topic_1', 'topic_2', 'topic_3',\n",
    "                       'topic_4', 'topic_5', 'topic_6','topic_7', 'topic_8', 'topic_9']\n",
    "\n",
    "# nossos transformadores com termos de interação sim e não\n",
    "poly_no_int = PolynomialFeatures(2, include_bias=False,interaction_only=False)\n",
    "poly_int = PolynomialFeatures(2, include_bias=False,interaction_only=True)\n",
    "\n",
    "# transformar os dados X do treino e teste\n",
    "dataframe_to_concat_train = []\n",
    "dataframe_to_concat_test = []\n",
    "for col in columns_to_poly:\n",
    "    dataframe_to_concat_train.append(pd.DataFrame(poly_no_int.fit_transform(x_train[col].values.reshape(-1,1)),\n",
    "                                           columns = [col,col+'**2']))\n",
    "    dataframe_to_concat_test.append(pd.DataFrame(poly_no_int.fit_transform(x_test[col].values.reshape(-1,1)),\n",
    "                                           columns = [col,col+'**2']))\n",
    "\n",
    "for col_poly in columns_to_poly:\n",
    "    for col_int in columns_to_interact:\n",
    "        k = poly_int.fit_transform(x_train[[col_poly,col_int]])\n",
    "        dataframe_to_concat_train.append(pd.DataFrame(poly_int.fit_transform(x_train[[col_poly,col_int]])[:,2],\n",
    "                                           columns = ['int_'+col_poly+'/'+col_int]))\n",
    "        dataframe_to_concat_test.append(pd.DataFrame(poly_int.fit_transform(x_test[[col_poly,col_int]])[:,2],\n",
    "                                           columns = ['int_'+col_poly+'/'+col_int]))        \n",
    "\n",
    "# adiciona as dummies\n",
    "dataframe_to_concat_train.append(x_train[columns_to_interact].reset_index(drop=True))\n",
    "dataframe_to_concat_test.append(x_test[columns_to_interact].reset_index(drop=True))\n",
    "\n",
    "# concatena todas nossas colunas\n",
    "x_train_poly = pd.concat(dataframe_to_concat_train, axis=1)\n",
    "x_test_poly = pd.concat(dataframe_to_concat_test, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cAetxeHWALpM"
   },
   "source": [
    "Com os dados modificados, podemos agora fitar o modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fmAa5lMtALpO",
    "outputId": "91034a10-bed7-4e64-c31d-a16e120f3b1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 0.5892116182572614, 'test': 0.5901639344262295}\n"
     ]
    }
   ],
   "source": [
    "x_train_t_poly = x_train_poly.as_matrix()\n",
    "x_test_t_poly = x_test_poly.as_matrix()\n",
    "\n",
    "lrPoly = LogisticRegressionCV(solver='liblinear', multi_class='ovr',\n",
    "                            penalty='l2', max_iter=100000, cv=10).fit(x_train_t_poly,y_train_t)\n",
    "\n",
    "lr_train_accuracy = lrPoly.score(x_train_t_poly, y_train_t)\n",
    "lr_test_accuracy = lrPoly.score(x_test_t_poly, y_test_t)\n",
    "\n",
    "scoresPoly = {'train': lr_train_accuracy, \n",
    "          'test': lr_test_accuracy}\n",
    "\n",
    "print(scoresPoly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S5uwiegwALpV"
   },
   "source": [
    "Podemos ver que o modelo teve uma performance melhor no treino do que o anterior, porém pior no teste. Isso provavelmente acontece devido ao overfitting que preditores polinomiais trazem.\n",
    "\n",
    "Duas outras medidas interessantes para se analisar nesse caso são:\n",
    "\n",
    "1) A porcentagem de valores $1$ (crescimento do índice) com relação ao total;\n",
    "2) A acurácia do modelo na classe $0$ (geralmente é a não dominante);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dMdo3YYGALpX",
    "outputId": "31f56e8c-65ed-47bc-8d62-07335ece7d86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia na classe 0: 0.2986111111111111 \n",
      "Porcentagem de 1s 0.5231788079470199\n"
     ]
    }
   ],
   "source": [
    "lr_class_accuracy = lrPoly.score(np.concatenate([x_train_t_poly[y_train_t==0,:],x_test_t_poly[y_test_t==0,:]]),\n",
    "                                                      np.concatenate([y_train_t[y_train_t==0],y_test_t[y_test_t==0]]))\n",
    "value_percentage = model_data[economic_index].sum()/len(model_data[economic_index])\n",
    "print(\"Acurácia na classe 0:\",lr_class_accuracy,\"\\nPorcentagem de 1s\",value_percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9SSRGBoqALpe"
   },
   "source": [
    "Podemos ver que nosso modelo possue baixa acurácia na classe $0$ (índice decresceu). Também é possível analisar que a acurácia de um modelo que só \"chutasse\" 1's seria quase a mesma do nosso modelo logístico, indicando que o modelo não conseguiu uma performance muito alta.\n",
    "\n",
    "Essas medidas correspondem apenas ao do modelo polinomial, mas poderíamos calcular também para o linear. Entretanto, como queremos analisar diversos outros fatores além destes, podemos partir para um algoritmo mais geral, isto é, podemos executar o mesmo procedimento para cada índice econômico diário, diversos valores de retardo $k$.\n",
    "\n",
    "Mas, antes disso, devemos ver quais índices iremos analisar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ipca</th>\n",
       "      <th>igpm</th>\n",
       "      <th>inpc</th>\n",
       "      <th>selic_meta</th>\n",
       "      <th>international_reserve</th>\n",
       "      <th>pnad</th>\n",
       "      <th>cdi</th>\n",
       "      <th>gdp</th>\n",
       "      <th>dollar</th>\n",
       "      <th>employment</th>\n",
       "      <th>gov_debt</th>\n",
       "      <th>consumer_confidence</th>\n",
       "      <th>ibovespa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>14.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>440.000000</td>\n",
       "      <td>304.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>304.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>304.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.177857</td>\n",
       "      <td>0.724286</td>\n",
       "      <td>0.198571</td>\n",
       "      <td>4.709659</td>\n",
       "      <td>365819.200658</td>\n",
       "      <td>11.923077</td>\n",
       "      <td>0.017924</td>\n",
       "      <td>135246.571429</td>\n",
       "      <td>4.464194</td>\n",
       "      <td>179.415714</td>\n",
       "      <td>41.007692</td>\n",
       "      <td>113.320714</td>\n",
       "      <td>1.410000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.366778</td>\n",
       "      <td>0.818993</td>\n",
       "      <td>0.367211</td>\n",
       "      <td>1.335170</td>\n",
       "      <td>16605.723326</td>\n",
       "      <td>0.679649</td>\n",
       "      <td>0.005106</td>\n",
       "      <td>21334.019762</td>\n",
       "      <td>0.608962</td>\n",
       "      <td>0.868655</td>\n",
       "      <td>1.486484</td>\n",
       "      <td>9.682220</td>\n",
       "      <td>2.415968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.380000</td>\n",
       "      <td>-0.670000</td>\n",
       "      <td>-0.250000</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>338789.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.008442</td>\n",
       "      <td>107306.000000</td>\n",
       "      <td>3.740000</td>\n",
       "      <td>178.290000</td>\n",
       "      <td>37.950000</td>\n",
       "      <td>96.820000</td>\n",
       "      <td>-0.670000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.285000</td>\n",
       "      <td>0.017500</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>351742.500000</td>\n",
       "      <td>11.600000</td>\n",
       "      <td>0.014227</td>\n",
       "      <td>118631.250000</td>\n",
       "      <td>4.031825</td>\n",
       "      <td>178.825000</td>\n",
       "      <td>40.690000</td>\n",
       "      <td>108.265000</td>\n",
       "      <td>0.085000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.580000</td>\n",
       "      <td>0.145000</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>363855.500000</td>\n",
       "      <td>11.800000</td>\n",
       "      <td>0.017089</td>\n",
       "      <td>135412.500000</td>\n",
       "      <td>4.172350</td>\n",
       "      <td>179.250000</td>\n",
       "      <td>41.160000</td>\n",
       "      <td>112.080000</td>\n",
       "      <td>0.840000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.257500</td>\n",
       "      <td>1.130000</td>\n",
       "      <td>0.272500</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>384912.750000</td>\n",
       "      <td>12.200000</td>\n",
       "      <td>0.022751</td>\n",
       "      <td>154674.250000</td>\n",
       "      <td>5.110750</td>\n",
       "      <td>180.030000</td>\n",
       "      <td>41.770000</td>\n",
       "      <td>120.605000</td>\n",
       "      <td>2.450000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.150000</td>\n",
       "      <td>2.230000</td>\n",
       "      <td>1.220000</td>\n",
       "      <td>6.500000</td>\n",
       "      <td>390510.000000</td>\n",
       "      <td>13.300000</td>\n",
       "      <td>0.024620</td>\n",
       "      <td>158836.000000</td>\n",
       "      <td>5.937200</td>\n",
       "      <td>180.660000</td>\n",
       "      <td>43.630000</td>\n",
       "      <td>131.790000</td>\n",
       "      <td>4.060000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ipca       igpm       inpc  selic_meta  international_reserve  \\\n",
       "count  14.000000  14.000000  14.000000  440.000000             304.000000   \n",
       "mean    0.177857   0.724286   0.198571    4.709659          365819.200658   \n",
       "std     0.366778   0.818993   0.367211    1.335170           16605.723326   \n",
       "min    -0.380000  -0.670000  -0.250000    2.250000          338789.000000   \n",
       "25%     0.025000   0.285000   0.017500    3.750000          351742.500000   \n",
       "50%     0.150000   0.580000   0.145000    4.500000          363855.500000   \n",
       "75%     0.257500   1.130000   0.272500    6.000000          384912.750000   \n",
       "max     1.150000   2.230000   1.220000    6.500000          390510.000000   \n",
       "\n",
       "            pnad         cdi            gdp      dollar  employment  \\\n",
       "count  13.000000  304.000000      14.000000  304.000000    7.000000   \n",
       "mean   11.923077    0.017924  135246.571429    4.464194  179.415714   \n",
       "std     0.679649    0.005106   21334.019762    0.608962    0.868655   \n",
       "min    11.000000    0.008442  107306.000000    3.740000  178.290000   \n",
       "25%    11.600000    0.014227  118631.250000    4.031825  178.825000   \n",
       "50%    11.800000    0.017089  135412.500000    4.172350  179.250000   \n",
       "75%    12.200000    0.022751  154674.250000    5.110750  180.030000   \n",
       "max    13.300000    0.024620  158836.000000    5.937200  180.660000   \n",
       "\n",
       "        gov_debt  consumer_confidence  ibovespa  \n",
       "count  13.000000            14.000000  3.000000  \n",
       "mean   41.007692           113.320714  1.410000  \n",
       "std     1.486484             9.682220  2.415968  \n",
       "min    37.950000            96.820000 -0.670000  \n",
       "25%    40.690000           108.265000  0.085000  \n",
       "50%    41.160000           112.080000  0.840000  \n",
       "75%    41.770000           120.605000  2.450000  \n",
       "max    43.630000           131.790000  4.060000  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "economic_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos observar, para este valor de $k$ e para o twitter de Jair bolsonaro, apenas alguns índices possuem uma quantidade útil de valores. Para outros casos isto irá continuar devido à continuidade dos dados. Portanto os índices de escolha são selic_meta, international_reserve, cdi e dollar. Com isso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IJUK-xI8ALpf",
    "outputId": "b7acc3a4-0b0b-43cf-fc65-d379dfda967d",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eco_ind</th>\n",
       "      <th>k</th>\n",
       "      <th>per_y1</th>\n",
       "      <th>LogLin_acu_tr</th>\n",
       "      <th>LogLin_acu_te</th>\n",
       "      <th>LogLin_cla0_acu</th>\n",
       "      <th>LogPoly_acu_tr</th>\n",
       "      <th>LogPoly_acu_te</th>\n",
       "      <th>LogPoly_cla0_acu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>selic_meta</td>\n",
       "      <td>0</td>\n",
       "      <td>0.981735</td>\n",
       "      <td>0.982857</td>\n",
       "      <td>0.977273</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.982857</td>\n",
       "      <td>0.977273</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>selic_meta</td>\n",
       "      <td>1</td>\n",
       "      <td>0.981735</td>\n",
       "      <td>0.982857</td>\n",
       "      <td>0.977273</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.982857</td>\n",
       "      <td>0.977273</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>selic_meta</td>\n",
       "      <td>2</td>\n",
       "      <td>0.981735</td>\n",
       "      <td>0.982857</td>\n",
       "      <td>0.977273</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.982857</td>\n",
       "      <td>0.977273</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>selic_meta</td>\n",
       "      <td>3</td>\n",
       "      <td>0.981735</td>\n",
       "      <td>0.982857</td>\n",
       "      <td>0.977273</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.982857</td>\n",
       "      <td>0.977273</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>selic_meta</td>\n",
       "      <td>4</td>\n",
       "      <td>0.981735</td>\n",
       "      <td>0.982857</td>\n",
       "      <td>0.977273</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.982857</td>\n",
       "      <td>0.977273</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cdi</td>\n",
       "      <td>0</td>\n",
       "      <td>0.973510</td>\n",
       "      <td>0.975104</td>\n",
       "      <td>0.967213</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.975104</td>\n",
       "      <td>0.967213</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cdi</td>\n",
       "      <td>1</td>\n",
       "      <td>0.973510</td>\n",
       "      <td>0.975104</td>\n",
       "      <td>0.967213</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.975104</td>\n",
       "      <td>0.967213</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cdi</td>\n",
       "      <td>2</td>\n",
       "      <td>0.973510</td>\n",
       "      <td>0.975104</td>\n",
       "      <td>0.967213</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.975104</td>\n",
       "      <td>0.967213</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>cdi</td>\n",
       "      <td>3</td>\n",
       "      <td>0.973510</td>\n",
       "      <td>0.975104</td>\n",
       "      <td>0.967213</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.975104</td>\n",
       "      <td>0.967213</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>cdi</td>\n",
       "      <td>4</td>\n",
       "      <td>0.973510</td>\n",
       "      <td>0.975104</td>\n",
       "      <td>0.967213</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.975104</td>\n",
       "      <td>0.967213</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>international_reserve</td>\n",
       "      <td>0</td>\n",
       "      <td>0.496689</td>\n",
       "      <td>0.564315</td>\n",
       "      <td>0.606557</td>\n",
       "      <td>0.572368</td>\n",
       "      <td>0.605809</td>\n",
       "      <td>0.508197</td>\n",
       "      <td>0.592105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>international_reserve</td>\n",
       "      <td>1</td>\n",
       "      <td>0.496689</td>\n",
       "      <td>0.593361</td>\n",
       "      <td>0.524590</td>\n",
       "      <td>0.572368</td>\n",
       "      <td>0.618257</td>\n",
       "      <td>0.590164</td>\n",
       "      <td>0.572368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>international_reserve</td>\n",
       "      <td>2</td>\n",
       "      <td>0.496689</td>\n",
       "      <td>0.605809</td>\n",
       "      <td>0.606557</td>\n",
       "      <td>0.598684</td>\n",
       "      <td>0.597510</td>\n",
       "      <td>0.639344</td>\n",
       "      <td>0.539474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>international_reserve</td>\n",
       "      <td>3</td>\n",
       "      <td>0.496689</td>\n",
       "      <td>0.572614</td>\n",
       "      <td>0.590164</td>\n",
       "      <td>0.572368</td>\n",
       "      <td>0.734440</td>\n",
       "      <td>0.508197</td>\n",
       "      <td>0.684211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>international_reserve</td>\n",
       "      <td>4</td>\n",
       "      <td>0.496689</td>\n",
       "      <td>0.605809</td>\n",
       "      <td>0.524590</td>\n",
       "      <td>0.559211</td>\n",
       "      <td>0.639004</td>\n",
       "      <td>0.557377</td>\n",
       "      <td>0.684211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>dollar</td>\n",
       "      <td>0</td>\n",
       "      <td>0.523179</td>\n",
       "      <td>0.597510</td>\n",
       "      <td>0.540984</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.709544</td>\n",
       "      <td>0.524590</td>\n",
       "      <td>0.631944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>dollar</td>\n",
       "      <td>1</td>\n",
       "      <td>0.523179</td>\n",
       "      <td>0.564315</td>\n",
       "      <td>0.573770</td>\n",
       "      <td>0.243056</td>\n",
       "      <td>0.572614</td>\n",
       "      <td>0.540984</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>dollar</td>\n",
       "      <td>2</td>\n",
       "      <td>0.523179</td>\n",
       "      <td>0.564315</td>\n",
       "      <td>0.557377</td>\n",
       "      <td>0.229167</td>\n",
       "      <td>0.589212</td>\n",
       "      <td>0.590164</td>\n",
       "      <td>0.298611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>dollar</td>\n",
       "      <td>3</td>\n",
       "      <td>0.523179</td>\n",
       "      <td>0.651452</td>\n",
       "      <td>0.524590</td>\n",
       "      <td>0.534722</td>\n",
       "      <td>0.564315</td>\n",
       "      <td>0.540984</td>\n",
       "      <td>0.229167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>dollar</td>\n",
       "      <td>4</td>\n",
       "      <td>0.523179</td>\n",
       "      <td>0.618257</td>\n",
       "      <td>0.508197</td>\n",
       "      <td>0.513889</td>\n",
       "      <td>0.726141</td>\n",
       "      <td>0.622951</td>\n",
       "      <td>0.659722</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  eco_ind  k    per_y1  LogLin_acu_tr  LogLin_acu_te  \\\n",
       "0              selic_meta  0  0.981735       0.982857       0.977273   \n",
       "1              selic_meta  1  0.981735       0.982857       0.977273   \n",
       "2              selic_meta  2  0.981735       0.982857       0.977273   \n",
       "3              selic_meta  3  0.981735       0.982857       0.977273   \n",
       "4              selic_meta  4  0.981735       0.982857       0.977273   \n",
       "5                     cdi  0  0.973510       0.975104       0.967213   \n",
       "6                     cdi  1  0.973510       0.975104       0.967213   \n",
       "7                     cdi  2  0.973510       0.975104       0.967213   \n",
       "8                     cdi  3  0.973510       0.975104       0.967213   \n",
       "9                     cdi  4  0.973510       0.975104       0.967213   \n",
       "10  international_reserve  0  0.496689       0.564315       0.606557   \n",
       "11  international_reserve  1  0.496689       0.593361       0.524590   \n",
       "12  international_reserve  2  0.496689       0.605809       0.606557   \n",
       "13  international_reserve  3  0.496689       0.572614       0.590164   \n",
       "14  international_reserve  4  0.496689       0.605809       0.524590   \n",
       "15                 dollar  0  0.523179       0.597510       0.540984   \n",
       "16                 dollar  1  0.523179       0.564315       0.573770   \n",
       "17                 dollar  2  0.523179       0.564315       0.557377   \n",
       "18                 dollar  3  0.523179       0.651452       0.524590   \n",
       "19                 dollar  4  0.523179       0.618257       0.508197   \n",
       "\n",
       "    LogLin_cla0_acu  LogPoly_acu_tr  LogPoly_acu_te  LogPoly_cla0_acu  \n",
       "0          0.000000        0.982857        0.977273          0.000000  \n",
       "1          0.000000        0.982857        0.977273          0.000000  \n",
       "2          0.000000        0.982857        0.977273          0.000000  \n",
       "3          0.000000        0.982857        0.977273          0.000000  \n",
       "4          0.000000        0.982857        0.977273          0.000000  \n",
       "5          0.000000        0.975104        0.967213          0.000000  \n",
       "6          0.000000        0.975104        0.967213          0.000000  \n",
       "7          0.000000        0.975104        0.967213          0.000000  \n",
       "8          0.000000        0.975104        0.967213          0.000000  \n",
       "9          0.000000        0.975104        0.967213          0.000000  \n",
       "10         0.572368        0.605809        0.508197          0.592105  \n",
       "11         0.572368        0.618257        0.590164          0.572368  \n",
       "12         0.598684        0.597510        0.639344          0.539474  \n",
       "13         0.572368        0.734440        0.508197          0.684211  \n",
       "14         0.559211        0.639004        0.557377          0.684211  \n",
       "15         0.458333        0.709544        0.524590          0.631944  \n",
       "16         0.243056        0.572614        0.540984          0.250000  \n",
       "17         0.229167        0.589212        0.590164          0.298611  \n",
       "18         0.534722        0.564315        0.540984          0.229167  \n",
       "19         0.513889        0.726141        0.622951          0.659722  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getModelResults(kMax,economic_index,economic_data,tweets_data_cla,model_bootstrap=False,only_linear=False):\n",
    "    # recebe os dados e parâmetros e executa todo o código apresentado até agora\n",
    "    dates = economic_data.index[~np.isnan(economic_data[economic_index])]\n",
    "    dates_index_tweets = [tweets_data_cla.index[row] in dates for row in range(0,len(tweets_data_cla))]\n",
    "    model_data = tweets_data_cla.iloc[dates_index_tweets,:]\n",
    "    model_data_values = model_data.values\n",
    "    for date_ind in range(0,len(model_data.index)):\n",
    "        row_values = model_data_values[date_ind,:]\n",
    "        values_found = 1\n",
    "        for k in range(1,kMax+1):\n",
    "            cur_date = model_data.index[date_ind] - timedelta(days=k)\n",
    "            if cur_date in tweets_data_cla.index:\n",
    "                row_values = row_values + tweets_data_cla.values[list(tweets_data_cla.index).index(cur_date),:]\n",
    "                values_found += 1\n",
    "        model_data_values[date_ind,:] = row_values/values_found\n",
    "    model_data = pd.DataFrame(model_data_values,columns=model_data.columns, index = model_data.index)\n",
    "    model_data[economic_index] = economic_data[economic_index][~np.isnan(economic_data[economic_index])]\n",
    "    eco_data = model_data[economic_index].values\n",
    "    for row in range(len(eco_data)-1,0,-1):\n",
    "        eco_data[row] = int(eco_data[row]>=eco_data[row-1])\n",
    "    model_data[economic_index] = eco_data\n",
    "    model_data = model_data.drop([model_data.index[0]])\n",
    "    data_train, data_test = train_test_split(model_data,\n",
    "                                             test_size = 0.2, \n",
    "                                             stratify=model_data[economic_index],random_state=42)\n",
    "    x_train = data_train.drop(columns=economic_index)\n",
    "    x_test = data_test.drop(columns=economic_index)\n",
    "    y_train = np.array(data_train[economic_index])\n",
    "    y_test = np.array(data_test[economic_index])        \n",
    "    \n",
    "    # caso queira gerar uma amostra bootstrap dos dados originais ao invés deles\n",
    "    # isso será utilizado na próxima subseção\n",
    "    if model_bootstrap:\n",
    "        x_train, y_train = make_bootstrap_sample(x_train,y_train)\n",
    "        y_train = y_train.ravel()\n",
    "    \n",
    "    x_train_t = x_train.as_matrix()\n",
    "    y_train_t = np.array(y_train)\n",
    "    x_test_t = x_test.as_matrix()\n",
    "    y_test_t = np.array(y_test)\n",
    "    lr = LogisticRegressionCV(solver='liblinear', multi_class='ovr',\n",
    "                        penalty='l2', max_iter=100000, cv=10).fit(x_train_t,y_train_t)\n",
    "    lr_train_accuracy = lr.score(x_train_t, y_train_t)\n",
    "    lr_test_accuracy = lr.score(x_test_t, y_test_t)\n",
    "    lr_class_accuracy = lr.score(np.concatenate([x_train_t[y_train_t==0,:],x_test_t[y_test_t==0,:]]),\n",
    "                                 np.concatenate([y_train_t[y_train_t==0],y_test_t[y_test_t==0]]))\n",
    "    scores = {'train': lr_train_accuracy,\n",
    "              'test': lr_test_accuracy,\n",
    "              'class_acu': lr_class_accuracy}\n",
    "    \n",
    "    # caso queiramos apenas o modelo linear, também será usado na próxima subseção\n",
    "    if only_linear:\n",
    "        return lr,np.array(x_train.columns)     \n",
    "    \n",
    "    columns_to_poly = ['retweet_count_mean', 'polarity_mean', 'subjectivity_mean','favorite_count_mean']\n",
    "    columns_to_interact = ['media_type_photo', 'media_type_video','topic_1', 'topic_2', 'topic_3',\n",
    "                           'topic_4', 'topic_5', 'topic_6','topic_7', 'topic_8', 'topic_9']\n",
    "    poly_no_int = PolynomialFeatures(2, include_bias=False,interaction_only=False)\n",
    "    poly_int = PolynomialFeatures(2, include_bias=False,interaction_only=True)\n",
    "    dataframe_to_concat_train = []\n",
    "    dataframe_to_concat_test = []\n",
    "    for col in columns_to_poly:\n",
    "        dataframe_to_concat_train.append(pd.DataFrame(poly_no_int.fit_transform(x_train[col].values.reshape(-1,1)),\n",
    "                                               columns = [col,col+'**2']))\n",
    "        dataframe_to_concat_test.append(pd.DataFrame(poly_no_int.fit_transform(x_test[col].values.reshape(-1,1)),\n",
    "                                               columns = [col,col+'**2']))\n",
    "    for col_poly in columns_to_poly:\n",
    "        for col_int in columns_to_interact:\n",
    "            k = poly_int.fit_transform(x_train[[col_poly,col_int]])\n",
    "            dataframe_to_concat_train.append(pd.DataFrame(poly_int.fit_transform(x_train[[col_poly,col_int]])[:,2],\n",
    "                                               columns = ['int_'+col_poly+'/'+col_int]))\n",
    "            dataframe_to_concat_test.append(pd.DataFrame(poly_int.fit_transform(x_test[[col_poly,col_int]])[:,2],\n",
    "                                               columns = ['int_'+col_poly+'/'+col_int])) \n",
    "    dataframe_to_concat_train.append(x_train[columns_to_interact].reset_index(drop=True))\n",
    "    dataframe_to_concat_test.append(x_test[columns_to_interact].reset_index(drop=True))\n",
    "    x_train_poly = pd.concat(dataframe_to_concat_train, axis=1)\n",
    "    x_test_poly = pd.concat(dataframe_to_concat_test, axis=1)\n",
    "    x_train_t_poly = x_train_poly.as_matrix()\n",
    "    x_test_t_poly = x_test_poly.as_matrix()\n",
    "    lrPoly = LogisticRegressionCV(solver='liblinear', multi_class='ovr',\n",
    "                                penalty='l2', max_iter=100000, cv=10).fit(x_train_t_poly,y_train_t)\n",
    "    lr_train_accuracy = lrPoly.score(x_train_t_poly, y_train_t)\n",
    "    lr_test_accuracy = lrPoly.score(x_test_t_poly, y_test_t)\n",
    "    lr_class_accuracy = lrPoly.score(np.concatenate([x_train_t_poly[y_train_t==0,:],x_test_t_poly[y_test_t==0,:]]),\n",
    "                                 np.concatenate([y_train_t[y_train_t==0],y_test_t[y_test_t==0]]))\n",
    "    scoresPoly = {'train': lr_train_accuracy,\n",
    "              'test': lr_test_accuracy,\n",
    "              'class_acu': lr_class_accuracy}\n",
    "    \n",
    "    # caso queiramos o polinomial\n",
    "    if model_bootstrap and not only_linear:\n",
    "        return lrPoly,np.array(x_train_poly.columns)\n",
    "    \n",
    "    # retorna uma lista com os resultados encontrados, sendo nessa lista os seguintes valores em ordem:\n",
    "    #'eco_ind','k','per_y1','LogLin_acu_tr','LogLin_acu_te',\n",
    "    #'LogLin_cla0_acu','LogPoly_acu_tr','LogPoly_acu_te','LogPoly_cla0_acu'\n",
    "    return([economic_index,kMax, \n",
    "                   model_data[economic_index].sum()/len(model_data[economic_index]),\n",
    "                   scores['train'],scores['test'],scores['class_acu'],\n",
    "                   scoresPoly['train'],scoresPoly['test'],scoresPoly['class_acu']])\n",
    "\n",
    "results = []\n",
    "for economic_index in ['selic_meta','cdi','international_reserve','dollar']:\n",
    "    for kMax in range(0,5):\n",
    "        results.append(getModelResults(kMax,economic_index,economic_data,tweets_data_cla))\n",
    "results = pd.DataFrame(results,columns=['eco_ind','k','per_y1','LogLin_acu_tr','LogLin_acu_te',\n",
    "                                       'LogLin_cla0_acu','LogPoly_acu_tr','LogPoly_acu_te',\n",
    "                                       'LogPoly_cla0_acu'])\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Como realizar a leitura do dataframe acima:**\n",
    "Cada linha corresponde a um valor de $k$ diferente ou um índice econômico diferente. Nossas colunas representam:\n",
    "1. eco_ind é o nosso índice econômico;\n",
    "2. $k$ é o nosso valor de retardo;\n",
    "3. per_y1 é a porcentagem de valores $1$ no nosso y, isto é, quantas vezes o índice e econômico cresceu com relação ao total;\n",
    "4. LogLin_acu_tr é a acurácia do modelo Logístico Linear no treino;\n",
    "5. LogLin_acu_te é a acurácia do modelo Logístico Linear no teste;\n",
    "6. LogLin_cla0_acu é a acurácia do modelo Logístico Linear na classe $0$ em ambos treino e teste;\n",
    "7. LogPoly_acu_tr é a acurácia do modelo Logístico Polinomial no treino;\n",
    "8. LogPoly_acu_te é a acurácia do modelo Logístico Polinomial no teste;\n",
    "9. LogPoly_cla0_acu é a acurácia do modelo Logístico Polinomial na classe $0$ em ambos treino e teste;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q5qCWHF0tQyr"
   },
   "source": [
    "O código acima é apenas tudo que fizermos anteriormente concatenados de forma à executarmos a mesma análise nos outros índices e valores de $k$. Uma análise a primeira instância é de que:\n",
    "\n",
    "1. Os índices econômicos selic_meta e cdi possuem 97% de valores $1$, de modo que nosso modelo falha pois classifica qualquer entrada em um aumento (valor $1$). Isso pode ser visto através da coluna de acurácia na classe $0$;\n",
    "2. Para a reserva internacional, $k=2$ nos forneceu a maior acurácia no teste em ambos modelos (linear e polinomial) com $0.606557$ e $0.639344$, respectivamente.\n",
    "3. Para o câmbio do dólar, $k=1$ nos forneceu a maior acurácia no teste no modelo $1$, com $0.573770$ enquanto $k=4$ no modelo $2$ nos deu $0.622951$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Significância dos coeficientes\n",
    "\n",
    "A partir dos três parâmetros juntamente com os índices que mostraram resultados interessantes anteriormente, esses são\n",
    "\n",
    "1. Para a reserva internacional $k=2$;\n",
    "2. Para o câmbio do dólar, $k=1$ para o modelo linear;\n",
    "3. Para o câmbio do dólar, $k=4$ no modelo polinomial;\n",
    "\n",
    "podemos analisar quais são os coeficientes que estão sendo mais significantes nos nossos modelos. O método funciona da seguinte maneira:\n",
    "\n",
    "- Primeiro geramos várias bootstrap samples e fitamos o modelo em cada uma delas;\n",
    "- Nisso guardamos os valores dos coeficientes em cada um dos bootstraps;\n",
    "- Considerando que os coeficientes vêm de uma distribuição normal, podemos verificar se o intervalo de $95%$ dessa distribuição contém o valor $0$. Se sim, esse coeficiente não é significante.\n",
    "\n",
    "Portanto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0%\n",
      "10%\n",
      "20%\n",
      "30%\n",
      "40%\n",
      "50%\n",
      "60%\n",
      "70%\n",
      "80%\n",
      "90%\n"
     ]
    }
   ],
   "source": [
    "## função para gerar uma amostra bootstrap aleatória\n",
    "def make_bootstrap_sample(dataset_X, dataset_y, size = None):\n",
    "    # dataset_x é o dataframe de X e dataset_y é um np.array(:,)\n",
    "    # o valor de retorno é uma amostra aleatória dos dados de tamanho igual à entrada\n",
    "    if not size: size = len(dataset_X)\n",
    "    if len(dataset_X) != len(dataset_y):\n",
    "        raise Exception(\"Data size must match between dataset_X and dataset_y\")\n",
    "    ind = np.random.randint(0,size,size)\n",
    "    bootstrap_dataset_X = dataset_X.iloc[ind,:]\n",
    "    bootstrap_dataset_y = np.array(dataset_y[ind])\n",
    "    bootstrap_dataset_y = bootstrap_dataset_y.reshape(-1,1)\n",
    "    return (bootstrap_dataset_X, bootstrap_dataset_y)\n",
    "\n",
    "def calculate_coefficients(columns_names, model):\n",
    "    # retorna o dicionário com a chave sendo o nome do preditor e\n",
    "    # o valor o coeficiente referente\n",
    "    values = np.array(model.coef_)\n",
    "    values = values.reshape(-1,1)\n",
    "    names = list(columns_names)\n",
    "    coefficients_dictionary = {names[i]:float(values[i]) for i in range(0,len(names))}\n",
    "    coefficients_dictionary['intercept'] = float(model.intercept_)\n",
    "    return coefficients_dictionary\n",
    "\n",
    "\n",
    "def get_significant_predictors(regression_coefficients, significance_level):\n",
    "    # regression coefficients é a lista de dicionários gerados pelo calculate_coefficients\n",
    "    # signifance_level é o grau de significância\n",
    "    # o retorno da função são os coeficientes que passam no teste de significância\n",
    "    sdCoef = norm.ppf(1-significance_level/2)\n",
    "    features = list(regression_coefficients[0].keys())\n",
    "    significant_coefficients = []\n",
    "    for feat in features:\n",
    "        values = [dic[feat] for dic in regression_coefficients]\n",
    "        mean = np.mean(values)\n",
    "        sdev = np.std(values)\n",
    "        if mean-sdCoef*sdev>0 or mean+sdCoef*sdev<0:\n",
    "            significant_coefficients.append(feat)\n",
    "    return significant_coefficients\n",
    "\n",
    "# valores para testar: k / economic_index / é o modelo linear?\n",
    "models = {1:[2,'international_reserve',True],\n",
    "           2:[2,'international_reserve',False],\n",
    "           3:[1,'dollar',True],\n",
    "           4:[4,'dollar',False]}\n",
    "\n",
    "models_sig_coe = {}\n",
    "\n",
    "# para cada modelo\n",
    "for i in range(1,len(models)+1):\n",
    "    k, economic_index, is_linear = models[i]\n",
    "    regression_coefficients = []\n",
    "    # rode n_samples=100 bootstraps e armezene os dicionários\n",
    "    n_samples = 100\n",
    "    for j in range(0,n_samples):\n",
    "        # uma fórmula para sabermos quanto falta\n",
    "        if(int((i-1)*n_samples+j)%(n_samples*len(models)/10)==0): print(str(round(int(((i-1)*n_samples+j)/(len(models))),2))+'%') \n",
    "        # pega o nosso modelo e o nome dos coeficientes\n",
    "        model,columns_names = getModelResults(k,economic_index,economic_data,\n",
    "                                              tweets_data_cla,model_bootstrap=True,only_linear=is_linear)\n",
    "        coefficients_dictionary = calculate_coefficients(columns_names, model)\n",
    "        # armazena na lista o dicionário\n",
    "        regression_coefficients.append(coefficients_dictionary)\n",
    "    # pegue os coeficientes significativos através dessa lista de dicionários\n",
    "    significance_level = 0.25\n",
    "    significant_coefficients = get_significant_predictors(regression_coefficients, significance_level)\n",
    "    models_sig_coe[i] = significant_coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: ['polarity_mean'],\n",
       " 2: ['subjectivity_mean**2',\n",
       "  'int_retweet_count_mean/topic_4',\n",
       "  'int_polarity_mean/media_type_photo',\n",
       "  'int_polarity_mean/topic_5',\n",
       "  'int_polarity_mean/topic_8',\n",
       "  'int_subjectivity_mean/media_type_photo',\n",
       "  'int_subjectivity_mean/topic_2',\n",
       "  'topic_4'],\n",
       " 3: ['polarity_mean'],\n",
       " 4: ['polarity_mean**2',\n",
       "  'subjectivity_mean**2',\n",
       "  'int_polarity_mean/topic_5',\n",
       "  'int_subjectivity_mean/topic_5',\n",
       "  'media_type_photo',\n",
       "  'media_type_video',\n",
       "  'topic_5',\n",
       "  'intercept']}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_sig_coe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtivemos os seguintes resultados:\n",
    "\n",
    "- Para os dois modelos lineares (1 e 3), a polaridade média é o único preditor com significância alfa;\n",
    "- Para o modelo polinomial, os tópicos mais influentes são o 2, 4, 5 e 8 na reserva internacional, juntamente com as dummies de mídia.\n",
    "- Para o modelo polinomial do câmbio do dólar, é interessante ver que ambas polaridade e subjetividade quadrática afetam o modelo polinomial, também interessante verificar que o tópico 0 (por causa do intercept) e 5 também são significantes."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "logistic_classification.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
